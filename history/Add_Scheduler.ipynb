{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjfghk5697/multi-label-classification-Mnist/blob/main/Add_Scheduler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 이진화 코드 추가\n",
        " 이진화란 이미지를 흰색과 검은색으로 바꾼다고 생각하면 된다.\n",
        "속도가 향상되고 정확도도 미약하지만 향상된것을 볼수 있다.\n",
        "\n",
        " [Dacon 적용 예시](https://dacon.io/competitions/official/235697/codeshare/2436?page=2&dtype=recent)\n",
        "\n",
        "\n",
        " [이진화 설명](https://gmnam.tistory.com/263)\n",
        "\n",
        " <hr>\n",
        "\n",
        " # 2. train, valid set 나누기\n",
        "```\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, num_workers=8)\n",
        "```\n",
        "처음에는 나누어지지 않아있는 코드였단 걸 발견하고 random_split 함수를 이용해 나눔. 그리고 학습 부분에서 valid 데이터를 enumerate하는 구문추가해서 valid 확인.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 3. Multi Sample Dropout 구현\n",
        "\n",
        "[자료](https://velog.io/@sujeongim/%EC%83%88%EB%A1%9C-%EB%B0%B0%EC%9A%B4-%EC%BD%94%EB%93%9C-%EC%A1%B0%EA%B0%81-Multi-Sample-Dropout)\n",
        "단순 dropout 기법을 안썼다. 근데 큰효과가 있는지는 잘 모르지만 overfitting의 위협도 없앨겸 선택했다.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 4. Scheduler 추가\n",
        "일단 간단하게 StepLR scheduler을 추가했다."
      ],
      "metadata": {
        "id": "mwCqIvW86tST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwV9U9iOB8M4",
        "outputId": "345999f0-34bb-49d9-ffcc-3374b368a852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/drive/MyDrive/Test image2\n",
        "\n",
        "#!unzip -qq \"/content/drive/MyDrive/test_dirty_mnist_2nd.zip\"\n",
        "\n",
        "%cd /content/drive/MyDrive/Train image2\n",
        "!unzip -qq \"/content/drive/MyDrive/dirty_mnist_2nd.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44kva0aP7lPz",
        "outputId": "7d2fb1ac-f44d-4b0d-e02a-3b29a2689a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Train image2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF4174-lYKPy"
      },
      "outputs": [],
      "source": [
        "!pip3 install torchinfo\n",
        "!pip3 install cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTMT6OMkBtpa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Tuple, Sequence, Callable\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50\n",
        "#이진화 추가\n",
        "import cv2\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMtnKh9Btpi"
      },
      "source": [
        "## 1. 커스텀 데이터셋 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjkhhJXmBtpl"
      },
      "outputs": [],
      "source": [
        "class MnistDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dir: os.PathLike,\n",
        "        image_ids: os.PathLike,\n",
        "        transforms: Sequence[Callable]\n",
        "    ) -> None:\n",
        "        self.dir = dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.labels = {}\n",
        "        with open(image_ids, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader)\n",
        "            for row in reader:\n",
        "                self.labels[int(row[0])] = list(map(int, row[1:]))\n",
        "\n",
        "        self.image_ids = list(self.labels.keys())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
        "        image_id = self.image_ids[index]\n",
        "        image = Image.open(\n",
        "            os.path.join(\n",
        "                self.dir, f'{str(image_id).zfill(5)}.png')).convert('RGB')\n",
        "        target = np.array(self.labels.get(image_id)).astype(np.float32)\n",
        "\n",
        "\n",
        "        image = np.array(image)\n",
        "#이진화 완료 cv2 툴을 써서 threshold로 254 값으로 변환. cv2는 넘파이만 사용 가능해서 넘파이로 변환후 fromarry로 pil로 바꾼다.\n",
        "        image_th = cv2.threshold(image,254,255,0)[1]\n",
        "        image_th = Image.fromarray(image_th) # NumPy array to PIL image\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image_th)\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoSfoQnxBtpo"
      },
      "source": [
        "## 2. 이미지 어그멘테이션"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ0iMdDKBtpp"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTvYYjNnBtpr"
      },
      "outputs": [],
      "source": [
        "trainset = MnistDataset('/content/drive/MyDrive/Train image2', '/content/drive/MyDrive/dirty_mnist_2nd_answer.csv', transforms_train)\n",
        "testset = MnistDataset('/content/drive/MyDrive/Test image2', '/content/drive/MyDrive/sample_submission.csv', transforms_test)\n",
        "\n",
        "train_size = int(0.8 * len(trainset))\n",
        "valid_size = len(trainset) - train_size\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=8)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, num_workers=8)\n",
        "\n",
        "test_loader = DataLoader(testset, batch_size=32, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJV1syjOBtps"
      },
      "source": [
        "## 3. ResNet50 모형"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kziKrm1tBtpt"
      },
      "outputs": [],
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "        self.classifier = nn.Linear(1000, 26)\n",
        "         # dropout samples 생성 \n",
        "        self.dropouts = nn.ModuleList([\n",
        "            nn.Dropout(0.5) for _ in range(5)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "\n",
        "#multi sample Dropout 구현. \n",
        "        for i,dropout in enumerate(self.dropouts):\n",
        "            if i== 0:\n",
        "                out = dropout(x.clone())\n",
        "                out = self.classifier(out)\n",
        "            else:\n",
        "                temp_out = dropout(x.clone())\n",
        "                out += self.classifier(temp_out)\n",
        "        return torch.sigmoid(out/len(self.dropouts))\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MnistModel().to(device)\n",
        "print(summary(model, input_size=(1, 3, 256, 256), verbose=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI_79bn8Btpw"
      },
      "source": [
        "## 4. 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ3j-bG4Btpy"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MultiLabelSoftMarginLoss()\n",
        "    #scheduler 추가 및 step 추가\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size = 5,\n",
        "                                                gamma = 0.75)\n",
        "num_epochs = 10\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_acc_list = []\n",
        "    valid_acc_list =[]\n",
        "    for i, (images, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            outputs = outputs > 0.5\n",
        "            acc = (outputs == targets).float().mean()\n",
        "            train_acc_list.append(acc)\n",
        "\n",
        "            print(f'{epoch}: \"train loss:\"{loss.item():.5f}, \"train acc: \"{acc.item():.5f}')\n",
        "    for i, (images, targets) in enumerate(valid_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        valid_loss = criterion(outputs, targets)\n",
        "      \n",
        "        if (i+1) % 10 == 0:\n",
        "            outputs = outputs > 0.5\n",
        "            valid_acc = (outputs == targets).float().mean()\n",
        "            valid_acc_list.append(acc)\n",
        "\n",
        "            print(f'{epoch}: \"val loss:\"{valid_loss.item():.5f}, \"val acc: \"{valid_acc.item():.5f}')\n",
        "    lr_scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeYvHfUtBtpz"
      },
      "source": [
        "## 5. 추론하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSfp-slSBtp0"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('data/sample_submission.csv')\n",
        "\n",
        "model.eval()\n",
        "batch_size = test_loader.batch_size\n",
        "batch_index = 0\n",
        "for i, (images, targets) in enumerate(test_loader):\n",
        "    images = images.to(device)\n",
        "    targets = targets.to(device)\n",
        "    outputs = model(images)\n",
        "    outputs = outputs > 0.5\n",
        "    batch_index = i * batch_size\n",
        "    submit.iloc[batch_index:batch_index+batch_size, 1:] = \\\n",
        "        outputs.long().squeeze(0).detach().cpu().numpy()\n",
        "    \n",
        "submit.to_csv('submit.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL0SuxUHUirr"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "\n",
        "plt.plot(train_acc_list)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "파이토치를_이용한 정말 간단한 Multi-Label Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}