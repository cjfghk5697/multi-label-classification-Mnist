{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjfghk5697/multi-label-classification-Mnist/blob/main/train%20Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwCqIvW86tST"
      },
      "source": [
        "# 1. 이진화 코드 추가\n",
        " 이진화란 이미지를 흰색과 검은색으로 바꾼다고 생각하면 된다.\n",
        "속도가 향상되고 정확도도 미약하지만 향상된것을 볼수 있다.\n",
        "\n",
        " [Dacon 적용 예시](https://dacon.io/competitions/official/235697/codeshare/2436?page=2&dtype=recent)\n",
        "\n",
        "\n",
        " [이진화 설명](https://gmnam.tistory.com/263)\n",
        "\n",
        " <hr>\n",
        "\n",
        " # 2. train, valid set 나누기\n",
        "```\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=8)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, num_workers=8)\n",
        "```\n",
        "처음에는 나누어지지 않아있는 코드였단 걸 발견하고 random_split 함수를 이용해 나눔. 그리고 학습 부분에서 valid 데이터를 enumerate하는 구문추가해서 valid 확인.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 3. Multi Sample Dropout 구현\n",
        "\n",
        "[자료](https://velog.io/@sujeongim/%EC%83%88%EB%A1%9C-%EB%B0%B0%EC%9A%B4-%EC%BD%94%EB%93%9C-%EC%A1%B0%EA%B0%81-Multi-Sample-Dropout)\n",
        "단순 dropout 기법을 안썼다. 근데 큰효과가 있는지는 잘 모르지만 overfitting의 위협도 없앨겸 선택했다.\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 4. Scheduler 추가\n",
        "일단 간단하게 StepLR scheduler을 추가했다.\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 5. Drop out 실수\n",
        "전 Drop out에서는 sigmoid가 있다. 근데 필자가 Multi-Label Loss Function을 이용함으로써 활성화 함수를 더 추가할 필요가 없었다. 그러나 sigmoid를 또 넣음으로써 sigmoid 계층 2개를 들어가는 거다. 결국 loss 값이 적어져서 학습 진행이 안된거다. 학습이 잘 안되면 활성화 함수에서 loss 값이 얼마 나오는지 잘 확인해야겠다..\n",
        "\n",
        "https://cvml.tistory.com/26 \n",
        "Multi-Label Loss Function에 대해 잘 설명 되어있다.\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "# 6. 왜 정확도가 안오를까?\n",
        "이상하게도 valid도 장확도가 같이 오르는데 test를 하면 정확도는 50%다 혹시나 해서 이진화를 빼고 해봤는데도 정확도가 안오른다 (05.15)\n",
        "\n",
        "-테스트\n",
        "\n",
        "1. efficientnetb1에서 epoch 수를 늘려봤다. 과거에는 5번 정도만 해도 valid 정확도가 높게 나왔다. 하지만 epoch를 늘린다고 해결될 일이 절대 아닌 거 같다.\n",
        "\n",
        "2. overfitting 인거 같아서 dropout을 추가했다. 활성화 함수도 한층 추가해봐서 실험. (실패)\n",
        "\n",
        "3. 추적그래프를 그리기 위해 추적그래프를 추가했다.\n",
        "\n",
        "4. DACON 토론에 올렸다.[링크](https://dacon.io/forum/406352)  답변이 왔고 확실히 여기서 model.eval()과 model.train(), model.zero_grab()에 대해 다시 공부했다.. valid에 zero_grab을 하여 학습이 진행되고 있었고 eval 또한 안해서 불필요한 과정을 가지고 있었다. \n",
        "\n",
        "### 해결\n",
        "결국은 inference에서 모델을 잘못 가지고 오고 있었다.. 결국 다시해본결과 80% 정확도가 나왔지만 아직 90% 정확도는 안나온다.\n",
        "\n",
        "# 7. 또다시 연구\n",
        "이번에는 적절한 파라미터를 찾기위한 튜닝 작업을 해보고자 한다.\n",
        "\n",
        "일단 Train 코드에서는 augmentation(mix up\n",
        ")과 여러 scheduler를 추가하려고한다. 이제 휴가가 끝나서 시간 날때마다 추가해야겠다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xwV9U9iOB8M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f67957-e5d3-4b62-a0a2-cf46723ee2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44kva0aP7lPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f77808f-45ce-4bdf-ceff-13d80cf73b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Train image2\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Train image2\n",
        "!unzip -q \"/content/drive/MyDrive/multi-mnist zipfile/dirty_mnist_2nd.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF4174-lYKPy"
      },
      "outputs": [],
      "source": [
        "!pip3 install torchinfo\n",
        "!pip3 install cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTMT6OMkBtpa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Tuple, Sequence, Callable\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchinfo import summary\n",
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_b0\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjkhhJXmBtpl"
      },
      "outputs": [],
      "source": [
        "class MnistDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dir: os.PathLike,\n",
        "        image_ids: os.PathLike,\n",
        "        transforms: Sequence[Callable]\n",
        "    ) -> None:\n",
        "        self.dir = dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.labels = {}\n",
        "        with open(image_ids, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader)\n",
        "            for row in reader:\n",
        "                self.labels[int(row[0])] = list(map(int, row[1:]))\n",
        "\n",
        "        self.image_ids = list(self.labels.keys())\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
        "        image_id = self.image_ids[index]\n",
        "        image = Image.open(\n",
        "            os.path.join(\n",
        "                self.dir, f'{str(image_id).zfill(5)}.png')).convert('RGB')\n",
        "        target = np.array(self.labels.get(image_id)).astype(np.float32)\n",
        "\n",
        "\n",
        "        #image = np.array(image)\n",
        "#이진화 완료 cv2 툴을 써서 threshold로 254 값으로 변환. cv2는 넘파이만 사용 가능해서 넘파이로 변환후 fromarry로 pil로 바꾼다.\n",
        "        #image_th = cv2.threshold(image,254,255,0)[1]\n",
        "        #image_th = Image.fromarray(image_th) # NumPy array to PIL image\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ0iMdDKBtpp"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTvYYjNnBtpr"
      },
      "outputs": [],
      "source": [
        "trainset = MnistDataset('/content/drive/MyDrive/Train image2', '/content/drive/MyDrive/multi-mnist zipfile/dirty_mnist_2nd_answer.csv', transforms_train)\n",
        "\n",
        "lengths = [int(len(trainset)*0.8), int(len(trainset)*0.2)]\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(trainset, lengths)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=8, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kziKrm1tBtpt"
      },
      "outputs": [],
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.model = efficientnet_b0(pretrained=True)\n",
        "        self.dropout=nn.Dropout(p=0.5)\n",
        "        self.classifier = nn.Linear(1000, 26)\n",
        "    def forward(self, x):\n",
        "        #x = F.relu(self.model(x))\n",
        "        x = self.model(x)  \n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MnistModel().to(device)\n",
        "print(summary(model, input_size=(1, 3, 256, 256), verbose=0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                e_w = p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "                self.state[p][\"e_w\"] = e_w\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        p.grad.norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm"
      ],
      "metadata": {
        "id": "eDxSO7GsATOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ3j-bG4Btpy"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "    #scheduler 추가 및 step 추가\n",
        "path = \"/content/drive/MyDrive/models/\"\n",
        "MODEL = \"06052218 eff4\"\n",
        "base_optimizer = torch.optim.SGD \n",
        "optimizer = SAM(model.parameters(), base_optimizer,lr=0.001, momentum=0.9)\n",
        "\n",
        "num_epochs = 33\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001, last_epoch=-1)\n",
        "\n",
        "valid_loss_list=[]\n",
        "train_loss_list=[]\n",
        "for epoch in range(num_epochs):\n",
        "    train_acc_list = []\n",
        "    valid_acc_list =[]\n",
        "    model.train()\n",
        "    for i, (images, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "        with torch.set_grad_enabled(True):\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.first_step(zero_grad=True)\n",
        "          criterion(model(images), targets).backward()\n",
        "          optimizer.second_step(zero_grad=True)\n",
        "          if (i+1) % 10 == 0:\n",
        "              outputs = outputs > 0.5\n",
        "              acc = (outputs == targets).cpu().float().mean()\n",
        "              train_acc_list.append(acc)\n",
        "              train_loss_list.append(loss)\n",
        "              print(f'{epoch}: \"train loss:\"{loss.item():.5f}, \"train acc: \"{acc.item():.5f}')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i, (images, targets) in enumerate(valid_loader):\n",
        "\n",
        "          images = images.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "          valid_loss = criterion(outputs, targets)\n",
        "      \n",
        "          if (i+1) % 10 == 0:\n",
        "              outputs = outputs > 0.5\n",
        "              valid_acc = (outputs == targets).cpu().float().mean()\n",
        "              valid_acc_list.append(valid_acc)\n",
        "              valid_loss_list.append(valid_loss)\n",
        "              print(f'{epoch}: \"val loss:\"{valid_loss.item():.5f}, \"val acc: \"{valid_acc.item():.5f}')\n",
        "    lr_scheduler.step()\n",
        "    torch.save(model.state_dict(), f'{path}_{MODEL}_1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL0SuxUHUirr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(train_acc_list,color='#e35f62', label='Train Acc')\n",
        "\n",
        "plt.xlabel('Acc')\n",
        "plt.ylabel('Epoch')\n",
        "plt.title('Train')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C67CRy0gHUiG"
      },
      "outputs": [],
      "source": [
        "plt.plot(valid_acc_list,color='green', label='Valid Acc')\n",
        "plt.xlabel('Acc')\n",
        "plt.ylabel('Epoch')\n",
        "plt.title('Valid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQacbQTr_5jh"
      },
      "source": [
        "# 기록\n",
        "1번 \n",
        "lr=1e-3\n",
        "\n",
        "1.   optimizer : Adam\n",
        "2.   scheduler : CosineAnnealingLR\n",
        "3.   criterition:MultiLabelSoftMarginLoss\n",
        "\n",
        "\n",
        "```\n",
        "9: \"val loss:\"0.57146, \"val acc: \"0.67067\n",
        "9: \"val loss:\"0.56269, \"val acc: \"0.67788\n",
        "9: \"val loss:\"0.56572, \"val acc: \"0.67308\n",
        "9: \"val loss:\"0.57164, \"val acc: \"0.66827\n",
        "9: \"val loss:\"0.54050, \"val acc: \"0.73077\n",
        "9: \"val loss:\"0.56166, \"val acc: \"0.66106\n",
        "9: \"val loss:\"0.53295, \"val acc: \"0.68990\n",
        "9: \"val loss:\"0.55628, \"val acc: \"0.68990\n",
        "9: \"val loss:\"0.52732, \"val acc: \"0.71394\n",
        "```\n",
        "\n",
        "2번\n",
        "1번과 같지만 모델이 efficientnet_b5 훨씬 효능이 좋다. \n",
        "\n",
        "```\n",
        "8: \"val loss:\"0.35732, \"val acc: \"0.83654\n",
        "8: \"val loss:\"0.40119, \"val acc: \"0.81250\n",
        "8: \"val loss:\"0.44620, \"val acc: \"0.77885\n",
        "8: \"val loss:\"0.38490, \"val acc: \"0.83173\n",
        "8: \"val loss:\"0.45323, \"val acc: \"0.81250\n",
        "8: \"val loss:\"0.48173, \"val acc: \"0.81731\n",
        "8: \"val loss:\"0.41693, \"val acc: \"0.81250\n",
        "8: \"val loss:\"0.36138, \"val acc: \"0.85577\n",
        "8: \"val loss:\"0.32753, \"val acc: \"0.85096\n",
        "8: \"val loss:\"0.39352, \"val acc: \"0.84615\n",
        "8: \"val loss:\"0.41519, \"val acc: \"0.81731\n",
        "8: \"val loss:\"0.39349, \"val acc: \"0.80769\n",
        "8: \"val loss:\"0.40276, \"val acc: \"0.82692\n",
        "8: \"val loss:\"0.40279, \"val acc: \"0.80288\n",
        "8: \"val loss:\"0.38277, \"val acc: \"0.82692\n",
        "8: \"val loss:\"0.35343, \"val acc: \"0.85577\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "파이토치를_이용한 정말 간단한 Multi-Label Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}